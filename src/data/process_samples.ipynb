{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import nest_asyncio\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from posteriordb import PosteriorDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults\n",
    "model_name = \"funnel10\"\n",
    "temp = \"../../\"  # delete when converting to Python module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"data\", \"raw\", model_name)\n",
    "posterior_path = os.path.join(\"posteriors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nuts_path(data_path):\n",
    "    all_dirs = os.listdir(os.path.join(temp, data_path))\n",
    "    \n",
    "    nuts_path = None\n",
    "    for dir in all_dirs:\n",
    "        if dir.startswith(\"nuts\"):\n",
    "            nuts_path = os.path.join(data_path, dir, \"chain_00\")\n",
    "            break\n",
    "        \n",
    "    if not nuts_path:\n",
    "        raise ValueError(\"No NUTS directory found in {}\".format(data_path))\n",
    "    return nuts_path\n",
    "    \n",
    "    \n",
    "def get_number_of_columns(csv_file_path):\n",
    "    \"\"\"\n",
    "    Get the number of columns in a tab-separated CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): Path to the tab-separated CSV file.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of columns in the CSV file.\n",
    "    \"\"\"\n",
    "    num_columns = 0\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file_path, 'r') as file:\n",
    "            csv_reader = csv.reader(file, delimiter='\\t')\n",
    "            first_row = next(csv_reader, None)\n",
    "            if first_row:\n",
    "                num_columns = len(first_row)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "    return num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nuts_df():\n",
    "\n",
    "    # Initialize an empty list to store data frames from each run\n",
    "    nuts_path = get_nuts_path(data_path)\n",
    "    nuts_path = os.path.join(temp, nuts_path)\n",
    "    nuts_list = []\n",
    "    num_columns = None\n",
    "\n",
    "    # Iterate through the run directories\n",
    "    for run_dir in os.listdir(nuts_path):\n",
    "        if run_dir.startswith('run_'):\n",
    "            run_number = int(run_dir.split('_')[1])\n",
    "            csv_path = os.path.join(nuts_path, run_dir, 'draws.csv')\n",
    "\n",
    "            if os.path.exists(csv_path):\n",
    "                # Add a \"run\" column with the run number\n",
    "                df = pd.read_csv(csv_path, sep='\\t', usecols=[\"stepsize__\", \"n_leapfrog__\"])\n",
    "                df['run'] = run_number\n",
    "                \n",
    "                # Get the total number of columns in the data frame\n",
    "                if not num_columns:\n",
    "                    num_columns = get_number_of_columns(csv_path)\n",
    "                \n",
    "                # Add the remaining columns by index starting from 11 (0-based)\n",
    "                remaining_columns = list(range(11, num_columns))\n",
    "                new_col_names = [f\"p{idx}\" for idx, val in enumerate(remaining_columns)]\n",
    "                \n",
    "                # Read the CSV file again for the remaining columns\n",
    "                remaining_df = pd.read_csv(csv_path, sep='\\t', usecols=remaining_columns)\n",
    "                \n",
    "                # Combine the dataframes by concatenating them\n",
    "                combined_df = pd.concat([df, remaining_df], axis=1)\n",
    "                nuts_list.append(combined_df)\n",
    "\n",
    "    # Concatenate all data frames into a single data frame\n",
    "    nuts_df = pd.concat(nuts_list, ignore_index=True)\n",
    "    return nuts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nuts_df(nuts_df):\n",
    "    # Step 1: Rename columns\n",
    "    nuts_df.rename(columns={'stepsize__': 'stepsize', 'n_leapfrog__': 'stepcount'}, inplace=True)\n",
    "\n",
    "    # Step 2: Rename arbitrary columns as p1, p2, etc.\n",
    "    for i in range(3, len(nuts_df.columns)):\n",
    "        nuts_df.rename(columns={nuts_df.columns[i]: f'p{i-3}'}, inplace=True)\n",
    "\n",
    "    # Step 3: Convert 'run' and 'stepcount' to unsigned int with minimal precision\n",
    "    nuts_df['run'] = pd.to_numeric(nuts_df['run'], downcast='unsigned')\n",
    "    nuts_df['stepcount'] = pd.to_numeric(nuts_df['stepcount'], downcast='unsigned')\n",
    "\n",
    "    # Step 4: Convert remaining columns to the smallest float data type\n",
    "    for col in nuts_df.columns[3:]:\n",
    "        nuts_df[col] = pd.to_numeric(nuts_df[col], downcast='float')\n",
    "\n",
    "    # Step 5: Reorder columns with 'run' as the first column, 'stepsize' as the second column, and 'stepcount' as the third column\n",
    "    column_order = ['run', 'stepsize', 'stepcount'] + list(nuts_df.columns.difference(['run', 'stepsize', 'stepcount']))\n",
    "    nuts_df = nuts_df[column_order]\n",
    "\n",
    "    # Step 6: Add a new column \"sampler\" with the value \"NUTS\" for every row\n",
    "    nuts_df.insert(0, \"sampler\", \"nuts\")\n",
    "    \n",
    "    \n",
    "    # convert \"stepsize\" column to float32 in Pandas dataframe\n",
    "    nuts_df = nuts_df.astype({'stepsize': 'float32'})\n",
    "    nuts_df = pl.from_pandas(nuts_df)\n",
    "    \n",
    "    return nuts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRHMC and DRGHMC With Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_draws(file_path):\n",
    "    draws_array = np.load(file_path)\n",
    "    draws_df = pl.from_numpy(draws_array)\n",
    "    \n",
    "    # rename columns as \"p1\", \"p2\", etc.\n",
    "    draws_df = draws_df.rename({c: f\"p{idx}\" for idx, c in enumerate(draws_df.columns)})\n",
    "    return draws_df\n",
    "\n",
    "def load_sampler_params(file_path, num_draws):\n",
    "    sampler_params_df = pl.read_json(file_path)\n",
    "    \n",
    "    sampler_params_df = sampler_params_df.select([\n",
    "        \"init_stepsize\",\n",
    "        \"reduction_factor\",\n",
    "        \"steps\",\n",
    "        \"dampening\", \n",
    "        \"num_proposals\",\n",
    "        \"probabilistic\", \n",
    "        \"sampler_type\", \n",
    "        \"grad_evals\"\n",
    "    ])\n",
    "    \n",
    "    schema={\n",
    "        \"init_stepsize\": pl.Float32, \n",
    "        \"reduction_factor\": pl.UInt8, \n",
    "        \"steps\": pl.Utf8, \n",
    "        \"dampening\": pl.Float32, \n",
    "        \"num_proposals\" :pl.UInt8, \n",
    "        \"probabilistic\": pl.Boolean,\n",
    "        \"grad_evals\": pl.UInt32\n",
    "    }\n",
    "    \n",
    "    sampler_params_df = sampler_params_df.with_columns(pl.col(c).cast(dtype) for c, dtype in schema.items())\n",
    "    \n",
    "    # Use concat to vertically stack the original DataFrame n times\n",
    "    sampler_params_repeated = pl.concat([sampler_params_df for _ in range(num_draws)])\n",
    "\n",
    "    return sampler_params_repeated\n",
    "\n",
    "def load_hyper_params(file_path, num_draws):\n",
    "    hyper_params_df = pl.read_json(file_path)\n",
    "    \n",
    "    hyper_params_df = hyper_params_df.select(\"global_seed\").rename({\"global_seed\": \"run\"})\n",
    "    \n",
    "    schema = {\"run\": pl.UInt8}\n",
    "    hyper_params_df = hyper_params_df.with_columns(pl.col(c).cast(dtype) for c, dtype in schema.items())\n",
    "    \n",
    "    hyper_params_repeated = pl.concat([hyper_params_df for _ in range(num_draws)])\n",
    "    \n",
    "    return hyper_params_repeated\n",
    "\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def helper(root, dir_name):\n",
    "    data = []\n",
    "\n",
    "    if dir_name.startswith(\"drhmc\") or dir_name.startswith(\"drghmc\"):\n",
    "        for run_dir in os.listdir(os.path.join(root, dir_name, \"chain_00\")):\n",
    "            if run_dir.startswith(\"run_\"):\n",
    "                draws_path = os.path.join(root, dir_name, \"chain_00\", run_dir, \"draws.npy\")\n",
    "                sampler_params_path = os.path.join(root, dir_name, \"chain_00\", run_dir, \"sampler_params.json\")\n",
    "                hyper_params_path = os.path.join(root, dir_name, \"chain_00\", run_dir, \"hyper_params.json\")\n",
    "                \n",
    "                if os.path.exists(draws_path) and os.path.exists(sampler_params_path) and os.path.exists(hyper_params_path):\n",
    "                    \n",
    "                    # Step 1\n",
    "                    draws_df = load_draws(draws_path)\n",
    "                    num_draws = draws_df.shape[0]\n",
    "\n",
    "                    # Step 2\n",
    "                    sampler_params_df = load_sampler_params(sampler_params_path, num_draws)\n",
    "\n",
    "                    # Step 3\n",
    "                    hyper_params_df = load_hyper_params(hyper_params_path, num_draws)\n",
    "                \n",
    "                    # Combine dataframes by repeating sampler_params and hyper_params for each row in draws_df\n",
    "                    combined_df = pl.concat([hyper_params_df, sampler_params_df, draws_df], how=\"horizontal\")\n",
    "\n",
    "                    data.append(combined_df)\n",
    "    return data\n",
    "    \n",
    "\n",
    "def samples_to_polars_df():\n",
    "    data = []\n",
    "    root, dirs, _ = next(os.walk(os.path.join(temp, data_path)))\n",
    "    \n",
    "    nest_asyncio.apply()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    looper =  asyncio.gather(*[helper(root, dir_name) for dir_name in dirs])\n",
    "    data = loop.run_until_complete(looper)\n",
    "    data = list(itertools.chain.from_iterable(data))\n",
    "    \n",
    "    samples = pl.concat(data)\n",
    "    samples = samples.rename({\n",
    "        \"sampler_type\": \"sampler\",\n",
    "        \"init_stepsize\": \"stepsize\",\n",
    "        \"steps\": \"stepcount\",\n",
    "    })\n",
    "    \n",
    "    # if \"stepcount\" column contains \"const_traj_len\", convert it to 0\n",
    "    samples = samples.with_columns(\n",
    "        pl.when(pl.col(\"stepcount\") == \"const_traj_len\").then(0).otherwise(pl.col(\"stepcount\")).alias(\"stepcount\")\n",
    "    )\n",
    "    \n",
    "    samples = samples.with_columns(pl.col(\"stepcount\").cast(pl.UInt16))\n",
    "    \n",
    "    \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ref_draws_json():\n",
    "    try:  # load posterior from custom model\n",
    "        path = os.path.join(temp, posterior_path, model_name)\n",
    "        ref_draws_path = os.path.join(path, f\"{model_name}.ref_draws.json.zip\")\n",
    "        ref_draws = json.load(zipfile.ZipFile(ref_draws_path).open(f\"{model_name}.ref_draws.json\"))\n",
    "        \n",
    "    except:  # try to load posterior from PDB\n",
    "        path = os.path.join(temp, posterior_path, \"posteriordb/posterior_database\")\n",
    "        pdb = PosteriorDatabase(path)\n",
    "        posterior = pdb.posterior(model_name)\n",
    "        ref_draws = posterior.reference_draws()\n",
    "        \n",
    "    return ref_draws\n",
    "\n",
    "def json_to_polars(ref_draws):\n",
    "    data = []\n",
    "    for idx, run in enumerate(ref_draws):\n",
    "        cur_df = pl.DataFrame(run)\n",
    "        cur_df = cur_df.rename({c: f\"p{idx}\" for idx, c in enumerate(cur_df.columns)})\n",
    "        \n",
    "        cur_df = cur_df.with_columns(pl.col(c).cast(pl.Float32) for c in cur_df.columns)\n",
    "        cur_df = cur_df.with_columns(pl.lit(idx).alias(\"run\"))\n",
    "        cur_df = cur_df.with_columns(pl.col(\"run\").cast(pl.UInt8))\n",
    "        \n",
    "        data.append(cur_df)\n",
    "    \n",
    "    # return Polars dataframe and add a column called \"sampler\" with value \"reference\"\n",
    "    return pl.concat(data).with_columns(pl.lit(\"ref\").alias(\"sampler\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (50_000, 12)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬─────┬─────────┐\n",
      "│ p0        ┆ p1        ┆ p2        ┆ p3        ┆ … ┆ p8        ┆ p9        ┆ run ┆ sampler │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ --- ┆ ---     │\n",
      "│ f32       ┆ f32       ┆ f32       ┆ f32       ┆   ┆ f32       ┆ f32       ┆ u8  ┆ str     │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═════╪═════════╡\n",
      "│ -4.811511 ┆ 0.597     ┆ -0.389606 ┆ 0.38073   ┆ … ┆ 0.569098  ┆ 0.484019  ┆ 0   ┆ ref     │\n",
      "│ 0.1923    ┆ -0.107716 ┆ 1.071435  ┆ -0.059764 ┆ … ┆ -0.603163 ┆ -1.967515 ┆ 0   ┆ ref     │\n",
      "│ 2.222674  ┆ 1.333205  ┆ 0.631023  ┆ 0.436719  ┆ … ┆ 0.074644  ┆ -2.415687 ┆ 0   ┆ ref     │\n",
      "│ 0.457858  ┆ -0.093025 ┆ 0.374908  ┆ -0.411884 ┆ … ┆ 0.140391  ┆ -0.149777 ┆ 0   ┆ ref     │\n",
      "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …   ┆ …       │\n",
      "│ 1.656757  ┆ -1.469907 ┆ 1.915202  ┆ 2.083553  ┆ … ┆ 0.876396  ┆ -1.733411 ┆ 49  ┆ ref     │\n",
      "│ -0.902077 ┆ -1.065768 ┆ -0.163906 ┆ 0.253706  ┆ … ┆ 0.30304   ┆ -1.778087 ┆ 49  ┆ ref     │\n",
      "│ -3.381415 ┆ 0.070828  ┆ -0.71026  ┆ -0.210306 ┆ … ┆ -0.322425 ┆ 0.677396  ┆ 49  ┆ ref     │\n",
      "│ 3.843143  ┆ 3.385573  ┆ -0.642328 ┆ -2.01088  ┆ … ┆ 0.560459  ┆ 3.790334  ┆ 49  ┆ ref     │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴─────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "ref_draws_df = json_to_polars(get_ref_draws_json())\n",
    "print(ref_draws_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dr_samplers = samples_to_polars_df()\n",
    "    nuts = process_nuts_df(create_nuts_df())\n",
    "    ref_draws = json_to_polars(get_ref_draws_json())\n",
    "\n",
    "    # Combine the two dataframes\n",
    "    samples = pl.concat([dr_samplers, nuts, ref_draws], how=\"diagonal\")\n",
    "    samples = samples.sort(\"run\")\n",
    "\n",
    "    # convert the \"sampler\" column to categorical\n",
    "    samples = samples.with_columns(pl.col(\"sampler\").cast(pl.Categorical))\n",
    "\n",
    "    samples = samples.select([\"run\", \"sampler\", \"stepcount\"] + diff(samples.columns, [\"run\", \"sampler\", \"stepcount\"]))\n",
    "\n",
    "    # save samples dataframe in compact format\n",
    "    path = os.path.join(temp, \"data/processed/samples.csv\")\n",
    "    samples.to_csv(path, sep=\"\\t\", use_column_names=True, null_value=\"NA\", n_threads=4)\n",
    "\n",
    "    path = os.path.join(temp, \"data/processed/\", model_name, \"samples.csv\")\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path))\n",
    "    samples.write_csv(path, separator=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drghmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
